{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Faster search with resizable embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import nomic.embed as embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's embed some text documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki100k = Dataset.from_generator(lambda: load_dataset('wikimedia/wikipedia', '20231101.en', split='train', streaming=True).take(100_000))\n",
    "wiki100k_embeddings = embed.text([row['text'][:1200] for row in wiki100k], model='nomic-embed-text-v1.5')['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "wiki100k_embeddings = np.array(wiki100k_embeddings)\n",
    "wiki100k_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And embed a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_query(query: str):\n",
    "    query = np.array(\n",
    "        embed.text(\n",
    "            [query],\n",
    "            model=\"nomic-embed-text-v1.5\",\n",
    "            task_type=\"search_query\",\n",
    "        )[\"embeddings\"]\n",
    "    )[0]\n",
    "    return query \n",
    "\n",
    "\n",
    "test_query = embed_query(\"tree data structures\")\n",
    "test_query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And search over our documents by comparing our query embedding with every other embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gene structure is the organisation of specialised sequence elements within a gene. Genes contain mos',\n",
       " 'In computer science, an AVL tree (named after inventors Adelson-Velsky and Landis) is a self-balanci',\n",
       " 'In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a ',\n",
       " 'An adaptive k-d tree is a tree for multidimensional points where successive levels may be split alon',\n",
       " 'In computer science, an (a,b) tree is a kind of balanced search tree.\\n\\nAn (a,b)-tree has all of its ',\n",
       " 'In computer science, a binary tree is a tree data structure in which each node has at most two child',\n",
       " 'XML documents have a hierarchical structure and can conceptually be interpreted as a tree structure,',\n",
       " 'In computer science, a B-tree is a self-balancing tree data structure that maintains sorted data and',\n",
       " 'TreeFam (Tree families database) is a database of phylogenetic trees of animal genes. It aims at dev',\n",
       " 'Tree Description Language (TreeDL) is a computer language for description of strictly-typed tree dat']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_scan(query, k=10): return np.argsort(query @ wiki100k_embeddings.T)[-k:]\n",
    "\n",
    "full_top10 = knn_scan(test_query)\n",
    "[doc[:100] for doc in wiki100k[full_top10]['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to compare a query embedding with 100,000 768-dimensional embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0454774563999672s per run / 21.988916688856886 runs/sec\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "def timed(f, n=100):    \n",
    "    ntime = timeit.timeit(f, number=n)\n",
    "    print(f'{ntime/n}s per run / {n/ntime} runs/sec')\n",
    "\n",
    "timed(lambda: knn_scan(test_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try smaller embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 512)\n"
     ]
    }
   ],
   "source": [
    "def truncated_scanner(embeddings, d):\n",
    "    truncated_embeddings = embeddings[:,:d]\n",
    "    truncated_embeddings /= np.linalg.norm(truncated_embeddings, axis=1)[np.newaxis].T\n",
    "    print(truncated_embeddings.shape)\n",
    "    def trunc_knn(query, k=10):\n",
    "        trunc_query = query[:d]\n",
    "        trunc_query /= np.linalg.norm(trunc_query)\n",
    "        return np.argsort(trunc_query @ truncated_embeddings.T)[-k:]\n",
    "    return trunc_knn\n",
    "\n",
    "knn_512d = truncated_scanner(wiki100k_embeddings, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_512d = knn_512d(test_query)\n",
    "len(set(top10_512d) & set(full_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032940823050012114s per run / 30.357468557532968 runs/sec\n"
     ]
    }
   ],
   "source": [
    "timed(lambda: knn_512d(test_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit faster and gets almost all of the same answers for our query! What if we go smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_256d = truncated_scanner(wiki100k_embeddings, 256)\n",
    "top10_256d = knn_256d(test_query)\n",
    "len(set(top10_256d) & set(full_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02735897060003481s per run / 36.55108281006478 runs/sec\n"
     ]
    }
   ],
   "source": [
    "timed(lambda: knn_256d(test_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even faster, but losing more accuracy now - we can recover most of this lost accuracy *and* keep most of the speedup by collecting extra smaller dimension results and *reranking* against the full size embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking_scanner(embeddings, d):\n",
    "    truncated_embeddings = embeddings[:,:d]\n",
    "    truncated_embeddings /= np.linalg.norm(truncated_embeddings, axis=1)[np.newaxis].T\n",
    "    def rerank_knn(query, k=10, expand=10):\n",
    "        expanded_k = k * expand\n",
    "        trunc_query = query[:d]\n",
    "        trunc_query /= np.linalg.norm(trunc_query)\n",
    "        candidate_indices = np.argsort(trunc_query @ truncated_embeddings.T)[-expanded_k:]\n",
    "        full_d_candidates = embeddings[candidate_indices]\n",
    "        return candidate_indices[np.argsort(query @ full_d_candidates.T)][-k:]\n",
    "    return rerank_knn\n",
    "\n",
    "rr_256d = reranking_scanner(wiki100k_embeddings, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_rr256d = rr_256d(test_query)\n",
    "len(set(top10_rr256d) & set(full_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03115591519002919s per run / 32.096633782082876 runs/sec\n"
     ]
    }
   ],
   "source": [
    "timed(lambda: rr_256d(test_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as fast, but we've gotten all our accuracy back!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
