{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437f5c15-b470-45f9-b8cb-3445dd0c88f7",
   "metadata": {},
   "source": [
    "# swisscoveryIA : recherche dans la collection de la bibliothèque de l'UNIGE en langage naturel\n",
    "\n",
    "Projet réalisé dans le cadre de Hackademia 2024, Battelle 22-23 novembre 2024  \n",
    "\n",
    "Auteurs : Antoine, Cédric, Abdoulaye, Nicolas Prongué (nicolas.prongue@unige.ch), Pablo Iriarte (pablo.iriarte@unige.ch)\n",
    "Date de création : 23.11.2024  \n",
    "Date de dernière modification : 22.11.2024  \n",
    "\n",
    "* LLM : LLAMA2 -> fichier llama-2-7b-chat.Q5_K_S.gguf de 4Gb disponible sur https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "* Briques techniques : LlamaCPP et LangChain -> bibliothèques open-source pour intéragir avec le LLM\n",
    "\n",
    "## Fonctionnement et tâches\n",
    "\n",
    "1. Classer la question posée dans une des deux catégories, recherche booléenne (BOOL) ou recherche conceptuelle (CONCEPT), selon la présence de certains mots dans la recherche (sac de mots)\n",
    "1. Réaliser un prompt qui entoure la question posée selon le type de recherche pour arriver au meilleur résultat possible\n",
    "1. Envoyer le prompt à l'IA installée en local\n",
    "1. Traiter la réponse donnée par l'IA pour obtenir:\n",
    "   1. BOOL : un fichier JSON avce les critères de récherche extraits par l'IA et séparés dans différents champs (auteur, date et sujet)\n",
    "   2. CONCEPT : un résumé fait par l'IA et des critères de recherche avec synonymes en français et anglais pour une recherche simple\n",
    "1. BOOL : Traiter la question pour ajouter un critère de recherche avec le type de document si certains mots sont présents dans la recherche (\"article\", \"livres\", etc.)\n",
    "2. Envoyer la requête à l'API de swisscovery pour obtenir les 10 documents les plus pertinants (critère de pertinance propre à swisscovery)\n",
    "3. Parser le fichier JSON de l'API swisscovery pour avoir une liste de résultats formatés pour les présenter sur l'interface\n",
    "4. Présenter le résultat :\n",
    "   1. BOOL : La liste de 10 résultats avec le lien sur chaque document vers swisscovery et donner un lien à la fin pour lancer la reqûete avancée sur swisscovery\n",
    "   2. CONCEPT : Le texte de l'IA suivi d'un disclaimer de la bibliothèque qui demande à la personne de vérifier l'information avec les documents trouvés sur swisscovery et donner un lien à la fin pour lancer la reqûete simple sur swisscovery\n",
    "\n",
    "## Résultats\n",
    "\n",
    "Interface Web simple avec :\n",
    "  * un entête\n",
    "  * un petit texte d'introduction\n",
    "  * un champ de recherche\n",
    "  * l'espace pour les résultats\n",
    "  * pied de page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceeff451-6f15-4b87-91a4-41e9bc8fbfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "# Paramètres\n",
    "swisscovery_api = 'https://api-eu.hosted.exlibrisgroup.com/primo/v1/search?vid=41SLSP_UGE:VU1&tab=41SLSP_UGE_MyInst_CI&search_scope=MyInst_and_CI'\n",
    "swisscovery_key = 'xxx' # Sandbox\n",
    "\n",
    "# Recherche pour les tests \n",
    "recherche = \"Je veux savoir ce qu'est le bosson de Higgs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05082a5a-1cb2-4dd4-a05d-e39d7587225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function de chat\n",
    "def chat(question):\n",
    "    # Load the LlamaCpp language model, adjust GPU usage based on your hardware\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"C:/Users/pablo/AI/llms/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "        temperature=0.5,\n",
    "        n_ctx=10000,\n",
    "        n_gpu_layers=-1,\n",
    "        n_batch=9000,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "        max_tokens=512,\n",
    "        n_threads=multiprocessing.cpu_count() - 1,\n",
    "        ## repeat_penalty=1.5,\n",
    "        # top_p=0.5,\n",
    "        verbose=False,  # Enable detailed logging for debugging\n",
    "    )\n",
    "    \n",
    "    # Define the prompt template with a placeholder for the question\n",
    "    template = \"\"\"\n",
    "    Question: I want to transform the question \"{question}\" into a new booleen query that I can use in a library catalog search.\n",
    "    Add several synonyms for the subjects and the translation of the terms into french and english and combine all the termes with OR.\n",
    "    Give me only the query without explanations and surrounded by [].\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    \n",
    "    # Create an LLMChain to manage interactions with the prompt and model\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    \n",
    "    print(\"Chatbot initialized, ready to chat...\")\n",
    "    while True:\n",
    "        # question = input(\"> \")\n",
    "        print(datetime.now()) \n",
    "        # answer = llm_chain.run(question)\n",
    "        answer = llm_chain.invoke(question)\n",
    "        # print(answer, '\\n')\n",
    "        print(\"Answer done\")\n",
    "        print (datetime.now())\n",
    "        return(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bac7d32-6a5a-463c-b88f-f9eca9a25721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_15632\\2345192473.py:34: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot initialized, ready to chat...\n",
      "2024-11-22 16:58:11.143428\n",
      "Answer done\n",
      "2024-11-22 16:58:44.858335\n"
     ]
    }
   ],
   "source": [
    "# chat()\n",
    "answer = chat(recherche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e2c373-cf59-41d2-beec-e3b155df9416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"Je veux savoir ce qu'est le bosson de Higgs\", 'text': ' [\"Je veux savoir ce qu\\'est le bosson de Higgs\" OR (\"matière noire\" OR \"particule de Higgs\") OR (\"boson de Higgs\" OR \"le boson de Higgs\")]'}\n"
     ]
    }
   ],
   "source": [
    "print (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da638779-bc15-4cac-8494-e50c04756604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Je veux savoir ce qu'est le bosson de Higgs\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d31880-4f7d-4016-a45d-9192fbb070d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [\"Je veux savoir ce qu\\'est le bosson de Higgs\" OR (\"matière noire\" OR \"particule de Higgs\") OR (\"boson de Higgs\" OR \"le boson de Higgs\")]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a14ddf67-4193-49eb-8740-15277814966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Je veux savoir ce qu'est le bosson de Higgs\" OR (\"matière noire\" OR \"particule de Higgs\") OR (\"boson de Higgs\" OR \"le boson de Higgs\")]\n"
     ]
    }
   ],
   "source": [
    "# extraction de la query\n",
    "query = answer['text']\n",
    "query = query.replace('`', '')\n",
    "query = query.replace('´', '')\n",
    "if (query[0] == '\\''):\n",
    "    query = query[1:]\n",
    "if (query[-1] == '\\''):\n",
    "    query = query[:-1]\n",
    "if (query[0] == ' '):\n",
    "    query = query[1:]\n",
    "if (query[-1] == ' '):\n",
    "    query = query[:-1]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc328aa3-19ff-4912-bd69-93567d6bc74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accolades OK\n",
      "\"Je veux savoir ce qu'est le bosson de Higgs\" OR (\"matière noire\" OR \"particule de Higgs\") OR (\"boson de Higgs\" OR \"le boson de Higgs\")\n"
     ]
    }
   ],
   "source": [
    "if len(query) > 0:\n",
    "    print (\"Accolades OK\")\n",
    "    query_clean = query[1:-1]\n",
    "print(query_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e7f22a3-5684-442e-9bdf-2c859114d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester si il y a du bla bla avant ou après\n",
    "expressions = ['note', 'recherche', 'search', 'français', 'anglais']\n",
    "for expression in expressions :\n",
    "    if expression in query.lower():\n",
    "        print(\"Warning, possible bla bla dans la requête!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf90015e-85cc-4558-88b2-6a0b3c8229bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api-eu.hosted.exlibrisgroup.com/primo/v1/search?vid=41SLSP_UGE:VU1&tab=CentralIndex&q=any,contains,little%5B%22Je+veux+savoir+ce+qu%27est+le+bosson+de+Higgs%22+OR+%28%22mati%C3%A8re+noire%22+OR+%22particule+de+Higgs%22%29+OR+%28%22boson+de+Higgs%22+OR+%22le+boson+de+Higgs%22%29%5D&apikey=l8xxc4695843cad14404b20ea33937905cb6\n"
     ]
    }
   ],
   "source": [
    "# convertir en URL\n",
    "import urllib.parse\n",
    "query_url = swisscovery_api + '&q=' + urllib.parse.quote_plus(query) + '&apikey=' + swisscovery_key\n",
    "print(query_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcc57c4-d3bf-406f-9b26-e8fd477b2d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[43mquery_url\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (r\u001b[38;5;241m.\u001b[39mjson())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_url' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "r = requests.get(query_url)\n",
    "print (r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2f93506-fe8f-4088-939f-874044f3d3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de résultats dans swisscovery : 84\n"
     ]
    }
   ],
   "source": [
    "# nombre de résultats\n",
    "sw_json = r.json()\n",
    "print ('Nombre de résultats dans swisscovery : ' + str(sw_json['info']['total']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb274d-c371-4682-b5f4-a9b97ffccea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
